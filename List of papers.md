|      | Paper title                                                  | Dataset                                                      | Preprocessing                                                | Algorithm                                                    | Experiment  Results                                          |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1    | Deep Audio-visual  Speech Recognition                        | 1.LRS2     2.LRS3                                            | 1.Generate  the dataset     2.Method of divide the dataset   | 1.Training  Strategy : Curriculum learning     2.Seq2seq      3.CTC | 1.Lip  only (Seq2seq better than CTC)     2.Out-of-sync Audio and Video (Seq2seq is better)     3. Seq2seq vs CTC       3.1Training time (CTC faster than  seq2seq)       3.2Inference time (CTC faster  than seq2seq) |
| 2    | Audio-Visual  Speech Recognition with a Hybrid      CTC/Attention Architecture(2018) | LRS2                                                         | Extract the mouth ROI from      the LRS2 dataset             | 1.ResNet       2.(B)LSTMs     3.CTC     4.RNN-LM             | Audio-visual  model(early fusion) is beteer      than audio-only model |
| 3    | Vggsound: A  lager-scale Audio-Visual Dataset                | Vggsound                                                     |                                                              | 1.Vggish  model      2.ResNet      3.NetVLAD                 | Propose  an automated pipeline for collecting a      large-scale audio-visual dataset – VGGSound. |
| 4    | Learning affective features with  a hybrid deep model for      audio–visual emotion recognition | The acted RML database     the acted eNTERFACE05 database     the     spontaneous BAUM-1s database | 1) Audio Input Generation     2) Visual Input Generation:    | CNN     3D-CNN     DBN                                       | hybrid deep learning     model jointly learns a discriminative audio-visual feature rep-     resentation, which performs better than previous hand-crafted     features and fusion methods on emotion recognition tasks. |
| 5    | A combined rule-based &  machine learning      audio-visual emotion recognition  approach |                                                              | face detection and  localization     Voice Activity Detector (VAD) | PCA     LDA     BDPCA     LSLDA                              |                                                              |
| 6    | Emotion recognition using deep  learning approach      from audio–visual emotional big data | The 2015 Emotion recognition sub-challenge dataset of static  facial expression | CNN                                                          | CNN     ELM                                                  |                                                              |
| 7    | [A novel feature set for video emotion recognition](https://proxy.library.spbu.ru:2068/science/article/pii/S092523121830198X) | VACAD，LIRIS-ACCEDE                                          | HHT and the cross-correlation technique                      | HHT                                                          | VACAD: The RMSE of SVR based on the proposed HHTC features is lower than the one based on the traditional features for all six emotions. Thus, the proposed features can outperform previous approaches with statistical significance.<br />LIRIS-ACCEDE: Thus, the computational load will be reduced by more than 70 times compared with the traditional feature extraction processes. |
| 8    | [Emotion Recognition Using Fusion of Audio and Video Features](https://proxy.library.spbu.ru:2281/abstract/document/8914655) | RECOLA<br />FER                                              | CNN                                                          | CNN                                                          |                                                              |
| 9    | [Video facial emotion recognition based on local enhanced motion history image and CNN-CTSLSTM networks](https://proxy.library.spbu.ru:2068/science/article/pii/S104732031830364X) | MMI<br />**CK+**<br />**AFEW**                               | (1) **Face detect**.<br />(2) **Face align**. (3) **Input normalization**. | Integrated framework of LEMHI-CNN and CNN-RNN                | Proposed framework achieves 93.9%, 78.4%, 51.2% accuracy on CK+, MMI, AFEW respectively. |
| 10   | [Ubiquitous Emotion Recognition Using Audio and Video Data](https://proxy.library.spbu.ru:2356/doi/abs/10.1145/3267305.3267689) | video BP4D+ multimodal data                                  | Video:<br />Use Haar features to detect the face and scale it 256*256<br />Audio:<br />plot the raw audio signal onto the 2D image plane and scale it 256*256 | CNN convolutional neural networks<br />Inception V3 CNN with 3 convolutional layers of size 32 64 and 128 | presented a method for recognizing emotion using audio and video data, including a method for representing raw audio signals as a plotted waveform.First, is to use the raw audio signals by splitting them into blocks of time and using this raw data to train our deep networks. Second, is the fusion of the modalities. This can be done by creating a new image from the face and audio images. This approach to image fusion has shown success in face recognition. |
| 11   | [Multiple Spatio-temporal Feature Learning for Video-based Emotion Recognition in the Wild](https://proxy.library.spbu.ru:2356/doi/abs/10.1145/3242969.3264992) |                                                              | BLSTM<br />CNN                                               | multiple spatio-temporal feature fusion (MSFF) framework<br />3-Dimensional Convolutional Neural Networks (3D CNN) | Extensive experiments show that the overall accuracy of our proposed MSFF is 60.64% |
|      |                                                              |                                                              |                                                              |                                                              |                                                              |


