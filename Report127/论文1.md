Learning Affective Features With a Hybrid Deep Model for Audio–Visual Emotion Recognition

使用混合深度模型学习情感特征以进行视听情感识别



特征提取 Feature extraction

目前的特征提取都是手工低级特征 不满足情感识别的需要

特征提取需要弥合“情感鸿沟”，需要提取有效区分情感的高级音频和视觉特征

多模态融合 Multimodality fusion

four typical fusion strategies: feature-level fusion, decision-level fusion, score-level fusion, and model-level fusion

四种典型的融合策略 特征级融合 决策级融合 分数级融合 模型级融合

需要设计更深层次的融合方法，为试听情感识别产生更优化的联合判别特征表示

two representative deep learning models are DBN [21] and CNN [22], [23], as described below.

两个具有代表性的深度学习模型 DBN CNN

本论文提出了由 CNN 3D-CNN DBN组成的混合深度学习框架 用于学习用于情感分类的联合视听特征表示

DBN用于融合视听特征 而不是对整个视频样本的情感进行分类

学习用于情感识别的视听特征是弥合情感鸿沟的关键步骤之一

**相关工作**

A 特征提取 

音频情感特征 韵律特征 语音质量特征 和频谱特征（MFCC 梅尔频率倒谱系数）

视频情感特征 

​	静态 基于外观的提取方法（CNN）

​	动态 面部动画参数或运动参数

B 多模态融合

多模态融合是整个具有不同统计特征的音频和视觉模态

现有的四种融合策略：特征级融合 决策级融合 分数级融合 模型级融合

特征级融合（早期融合

​	所有提取的特征直接连接成一个单一的高维特征向量 利用此高维特征向量训练单个分类器进行情感识别 由于直接融合特征 不能模拟复杂的关系 

决策级融合（后期融合

​	通过代数组合规则多个单峰情感识别结果 无法捕捉不同模态之间的相互关联 因为这些模态被假定为独立的 决策级融合不符合人类以互补冗余的方式

分数级融合（决策级融合的变体

​	对获得的类分值采用等权求和。 将这个融合后的得分向量中最大值对应的情感类别作为最终的预测类别。

模型级融合（特征级融合+决策级融合

​	该方法旨在获得音频和视觉模态的联合特征表示。 它的实现主要取决于使用的融合模型。

C 总结

1 开发自动特征学习算法来获取高级情感特征

2 开发利用深度模型进行特征融合的深度融合算法

III. PROPOSED METHOD

音频模型使用CNN模型处理音频信号

视觉网络使用3D-CNN模型处理视觉数据

两个网络的全连接层的输出融合在一个用DBN模型构建的融合网络中

A. Generation of Network Inputs

B. Network Training

C. Emotion Classiﬁcation

在完成融合网络的训练后，可以在每个视听片段上计算一个 2048 维的联合特征表示。 由于每个视听视频样本具有不同数量的片段，因此对来自每个视频样本的所有片段特征应用平均池化以形成固定长度的全局视频特征表示。 我们的实验比较了平均池和最大池，发现平均池表现更好。 因此，我们采用平均池来处理从片段中提取的特征。 基于这种全局视频特征表示，线性支持向量机分类器可以很容易地用于情感识别。

IV. EXPERIMENTS







